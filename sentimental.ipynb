{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.7267\n",
      "Epoch [2/100], Train Loss: 2.7723\n",
      "Epoch [3/100], Train Loss: 0.5921\n",
      "Epoch [4/100], Train Loss: 0.5711\n",
      "Epoch [5/100], Train Loss: 0.5445\n",
      "Epoch [6/100], Train Loss: 0.4359\n",
      "Epoch [7/100], Train Loss: 0.2956\n",
      "Epoch [8/100], Train Loss: 0.1414\n",
      "Epoch [9/100], Train Loss: 0.3297\n",
      "Epoch [10/100], Train Loss: 0.4926\n",
      "Epoch [11/100], Train Loss: 0.4521\n",
      "Epoch [12/100], Train Loss: 0.0452\n",
      "Epoch [13/100], Train Loss: 0.1285\n",
      "Epoch [14/100], Train Loss: 0.2135\n",
      "Epoch [15/100], Train Loss: 0.0688\n",
      "Epoch [16/100], Train Loss: 0.0195\n",
      "Epoch [17/100], Train Loss: 0.0202\n",
      "Epoch [18/100], Train Loss: 0.0235\n",
      "Epoch [19/100], Train Loss: 0.0314\n",
      "Epoch [20/100], Train Loss: 0.0222\n",
      "Epoch [21/100], Train Loss: 0.0158\n",
      "Epoch [22/100], Train Loss: 0.0110\n",
      "Epoch [23/100], Train Loss: 0.0092\n",
      "Epoch [24/100], Train Loss: 0.0073\n",
      "Epoch [25/100], Train Loss: 0.0065\n",
      "Epoch [26/100], Train Loss: 0.0056\n",
      "Epoch [27/100], Train Loss: 0.0048\n",
      "Epoch [28/100], Train Loss: 0.0045\n",
      "Epoch [29/100], Train Loss: 0.0062\n",
      "Epoch [30/100], Train Loss: 0.0035\n",
      "Epoch [31/100], Train Loss: 0.0032\n",
      "Epoch [32/100], Train Loss: 0.0029\n",
      "Epoch [33/100], Train Loss: 0.0026\n",
      "Epoch [34/100], Train Loss: 0.0024\n",
      "Epoch [35/100], Train Loss: 0.0022\n",
      "Epoch [36/100], Train Loss: 0.0021\n",
      "Epoch [37/100], Train Loss: 0.0019\n",
      "Epoch [38/100], Train Loss: 0.0018\n",
      "Epoch [39/100], Train Loss: 0.0017\n",
      "Epoch [40/100], Train Loss: 0.0016\n",
      "Epoch [41/100], Train Loss: 0.0015\n",
      "Epoch [42/100], Train Loss: 0.0014\n",
      "Epoch [43/100], Train Loss: 0.0014\n",
      "Epoch [44/100], Train Loss: 0.0013\n",
      "Epoch [45/100], Train Loss: 0.0013\n",
      "Epoch [46/100], Train Loss: 0.0012\n",
      "Epoch [47/100], Train Loss: 0.0012\n",
      "Epoch [48/100], Train Loss: 0.0011\n",
      "Epoch [49/100], Train Loss: 0.0011\n",
      "Epoch [50/100], Train Loss: 0.0010\n",
      "Epoch [51/100], Train Loss: 0.0010\n",
      "Epoch [52/100], Train Loss: 0.0010\n",
      "Epoch [53/100], Train Loss: 0.0010\n",
      "Epoch [54/100], Train Loss: 0.0009\n",
      "Epoch [55/100], Train Loss: 0.0009\n",
      "Epoch [56/100], Train Loss: 0.0009\n",
      "Epoch [57/100], Train Loss: 0.0008\n",
      "Epoch [58/100], Train Loss: 0.0008\n",
      "Epoch [59/100], Train Loss: 0.0008\n",
      "Epoch [60/100], Train Loss: 0.0008\n",
      "Epoch [61/100], Train Loss: 0.0008\n",
      "Epoch [62/100], Train Loss: 0.0007\n",
      "Epoch [63/100], Train Loss: 0.0007\n",
      "Epoch [64/100], Train Loss: 0.0007\n",
      "Epoch [65/100], Train Loss: 0.0007\n",
      "Epoch [66/100], Train Loss: 0.0007\n",
      "Epoch [67/100], Train Loss: 0.0007\n",
      "Epoch [68/100], Train Loss: 0.0007\n",
      "Epoch [69/100], Train Loss: 0.0007\n",
      "Epoch [70/100], Train Loss: 0.0006\n",
      "Epoch [71/100], Train Loss: 0.0007\n",
      "Epoch [72/100], Train Loss: 0.0006\n",
      "Epoch [73/100], Train Loss: 0.0006\n",
      "Epoch [74/100], Train Loss: 0.0006\n",
      "Epoch [75/100], Train Loss: 0.0006\n",
      "Epoch [76/100], Train Loss: 0.0006\n",
      "Epoch [77/100], Train Loss: 0.0006\n",
      "Epoch [78/100], Train Loss: 0.0006\n",
      "Epoch [79/100], Train Loss: 0.0006\n",
      "Epoch [80/100], Train Loss: 0.0006\n",
      "Epoch [81/100], Train Loss: 0.0005\n",
      "Epoch [82/100], Train Loss: 0.0006\n",
      "Epoch [83/100], Train Loss: 0.0005\n",
      "Epoch [84/100], Train Loss: 0.0005\n",
      "Epoch [85/100], Train Loss: 0.0005\n",
      "Epoch [86/100], Train Loss: 0.0005\n",
      "Epoch [87/100], Train Loss: 0.0005\n",
      "Epoch [88/100], Train Loss: 0.0005\n",
      "Epoch [89/100], Train Loss: 0.0005\n",
      "Epoch [90/100], Train Loss: 0.0005\n",
      "Epoch [91/100], Train Loss: 0.0005\n",
      "Epoch [92/100], Train Loss: 0.0005\n",
      "Epoch [93/100], Train Loss: 0.0005\n",
      "Epoch [94/100], Train Loss: 0.0005\n",
      "Epoch [95/100], Train Loss: 0.0005\n",
      "Epoch [96/100], Train Loss: 0.0005\n",
      "Epoch [97/100], Train Loss: 0.0005\n",
      "Epoch [98/100], Train Loss: 0.0005\n",
      "Epoch [99/100], Train Loss: 0.0005\n",
      "Epoch [100/100], Train Loss: 0.0005\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Example text data\n",
    "text_data = [\n",
    "    \"I love this movie!\", \"This movie is terrible.\", \"The acting was amazing.\",\n",
    "    \"I hate this product.\", \"The food was delicious.\", \"I'm not satisfied with the service.\",\n",
    "    \"This book is fantastic!\", \"The customer support was awful.\", \"I enjoyed my vacation.\",\n",
    "    \"The quality of the product is poor.\", \"The movie made me cry.\", \"I feel great after watching it!\",\n",
    "    \"The product exceeded my expectations.\", \"I wouldn't recommend it to anyone.\",\n",
    "    \"I'm impressed with the performance.\", \"The experience was disappointing.\",\n",
    "    \"The service was exceptional!\", \"I regret buying this.\", \"Best movie I've ever seen!\",\n",
    "    \"This was a waste of money.\", \"I'm thrilled with my purchase.\", \"The worst experience ever.\",\n",
    "    \"Absolutely amazing!\", \"A total disappointment.\", \"Highly recommended!\",\n",
    "    \"Couldn't be more dissatisfied.\", \"So much fun!\", \"Complete waste of time.\",\n",
    "    \"Brilliant performance!\", \"An utter disaster.\"\n",
    "]\n",
    "\n",
    "# Corresponding sentiment labels (1 for positive sentiment, 0 for negative sentiment)\n",
    "sentiment_labels = torch.tensor(\n",
    "    [1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0,\n",
    "     1, 0, 1, 0, 1, 0, 1, 0, 1, 0], dtype=torch.float32)\n",
    "\n",
    "# Tokenize and build vocabulary\n",
    "tokenized_data = [word_tokenize(sentence.lower()) for sentence in text_data]\n",
    "vocab = {word: idx for idx, word in enumerate(set(word for sentence in tokenized_data for word in sentence))}\n",
    "index_data = [[vocab[word] for word in sentence] for sentence in tokenized_data]\n",
    "\n",
    "# Convert to tensors and pad sequences\n",
    "tensor_data = [torch.tensor(sentence) for sentence in index_data]\n",
    "padded_data = pad_sequence(tensor_data, batch_first=True, padding_value=0)\n",
    "max_length = padded_data.size(1)\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.head_dim = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, _ = x.size()\n",
    "        x = x.view(batch_size, seq_length, self.num_heads, self.head_dim)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size, num_heads, seq_length, _ = x.size()\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        return x.view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V):\n",
    "        Q_transformed = self.W_q(Q)\n",
    "        K_transformed = self.W_k(K)\n",
    "        V_transformed = self.W_v(V)\n",
    "\n",
    "        Q_split = self.split_heads(Q_transformed)\n",
    "        K_split = self.split_heads(K_transformed)\n",
    "        V_split = self.split_heads(V_transformed)\n",
    "\n",
    "        attention_scores = torch.matmul(Q_split, K_split.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_output = torch.matmul(attention_weights, V_split)\n",
    "\n",
    "        attention_output = self.combine_heads(attention_output)\n",
    "        output = self.W_out(attention_output)\n",
    "        return output\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_length, d_model)\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiheadAttention(d_model, num_heads)\n",
    "        self.feedforwardnn = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_mask=None):\n",
    "        attention_output = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        ff_output = self.feedforwardnn(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, num_heads, d_ff, num_layers, dropout, max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_length)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        src = self.embedding(src)\n",
    "        src = self.pos_encoding(src)\n",
    "        src = self.dropout(src)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        \n",
    "        return src\n",
    "\n",
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, encoder, d_model, num_classes=1):\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        encoder_output = self.encoder(src, src_mask)\n",
    "        pooled_output = encoder_output.mean(dim=1)  # Global average pooling\n",
    "        output = self.fc(pooled_output)\n",
    "        return output\n",
    "\n",
    "# Model hyperparameters\n",
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT = 0.1\n",
    "MAX_LENGTH = max_length\n",
    "\n",
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, NUM_HEADS, HIDDEN_DIM, NUM_LAYERS, DROPOUT, MAX_LENGTH)\n",
    "model = SentimentClassifier(encoder, EMBEDDING_DIM)\n",
    "\n",
    "# Specify optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TensorDataset(padded_data, sentiment_labels)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Evaluation loop\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            predictions = model(text).squeeze(1)\n",
    "            loss = criterion(predictions, labels)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# Training\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample text for testing\n",
    "test_text = \"I not like this car\"\n",
    "\n",
    "# Preprocess the input text\n",
    "def preprocess_text(text, vocab, max_length):\n",
    "    tokenized = word_tokenize(text.lower())\n",
    "    indexed = [vocab.get(word, 0) for word in tokenized]  # Use 0 for unknown words\n",
    "    tensor = torch.tensor(indexed, dtype=torch.long)\n",
    "    padded = torch.nn.functional.pad(tensor, (0, max_length - len(tensor)), value=0)  # Pad to max_length\n",
    "    return padded\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Preprocess the test text\n",
    "test_tensor = preprocess_text(test_text, vocab, max_length).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Move the tensor to the appropriate device\n",
    "test_tensor = test_tensor.to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Perform the prediction\n",
    "with torch.no_grad():\n",
    "    output = model(test_tensor)\n",
    "    prediction = torch.sigmoid(output).item()  # Apply sigmoid to get a probability\n",
    "\n",
    "# Print the prediction\n",
    "if prediction >= 0.5:\n",
    "    print(\"Positive sentiment\")\n",
    "else:\n",
    "    print(\"Negative sentiment\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
